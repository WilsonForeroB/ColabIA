{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDcS/89eh/aTDToaLXQ61N"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LLMs Locales en la Nube: Ollama + RAG con LangChain**\n",
        "\n",
        "**Objetivo de la clase:**\n",
        "Aprender a levantar un servidor de LLMs en Google Colab (GPU gratuita),\n",
        "exponerlo al exterior con ngrok, y construir un sistema RAG (Retrieval-Augmented\n",
        "Generation) con LangChain que responda preguntas sobre documentos propios.\n",
        "\n",
        "## ¬øQu√© es RAG y por qu√© importa?\n",
        "\n",
        "Un LLM \"de base\" solo conoce lo que vio durante su entrenamiento.\n",
        "RAG resuelve esto en 3 pasos:\n",
        "\n",
        "  1. üìÑ **Indexar** tus documentos (convertirlos en vectores)\n",
        "  2. üîç **Recuperar** los fragmentos m√°s relevantes para cada pregunta\n",
        "  3. üí¨ **Generar** una respuesta usando esos fragmentos como contexto\n",
        "\n",
        "```\n",
        "Pregunta ‚îÄ‚îÄ‚ñ∫ Recuperador ‚îÄ‚îÄ‚ñ∫ Fragmentos relevantes ‚îÄ‚îÄ‚ñ∫ LLM ‚îÄ‚îÄ‚ñ∫ Respuesta\n",
        "                  ‚ñ≤\n",
        "            Base vectorial\n",
        "            (tus documentos)\n",
        "```\n",
        "\n",
        "## Requisitos previos\n",
        "- Cuenta Google con acceso a Colab\n",
        "- Cuenta ngrok gratuita: https://dashboard.ngrok.com/signup\n",
        "- Auth Token de ngrok: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "\n",
        "‚ö†Ô∏è Antes de empezar: **Runtime ‚Üí Change runtime type ‚Üí T4 GPU**"
      ],
      "metadata": {
        "id": "SN-aKKgm26lm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VO0PVctz2bb7",
        "outputId": "13e4dced-99a4-4ee7-fbf0-7318924bdb81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU disponible:\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# PASO 0: Verificar GPU\n",
        "# ===========================================================================\n",
        "# Siempre verificamos primero que tenemos GPU disponible.\n",
        "# Sin GPU, los modelos tardan 10-20x m√°s ‚Üí clase inviable.\n",
        "\n",
        "import subprocess\n",
        "\n",
        "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "if result.returncode != 0:\n",
        "    raise RuntimeError(\n",
        "        \"‚ùå No se detect√≥ GPU.\\n\"\n",
        "        \"Ve a Runtime ‚Üí Change runtime type ‚Üí T4 GPU y vuelve a ejecutar.\"\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ GPU disponible:\")\n",
        "print(result.stdout.split('\\n')[8])  # L√≠nea con el modelo de GPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===========================================================================\n",
        "# PASO 1: Instalar Ollama\n",
        "# ===========================================================================\n",
        "# Ollama es un servidor que gestiona modelos LLM localmente.\n",
        "# Expone una API REST en el puerto 11434, compatible con la API de OpenAI.\n",
        "#\n",
        "# Dependencias del sistema:\n",
        "#   - zstd: para descomprimir el binario de Ollama\n",
        "#   - pciutils: para que Ollama detecte la GPU (¬°sin esto corre en CPU!)\n",
        "\n",
        "import os\n",
        "os.environ['DEBIAN_FRONTEND'] = 'noninteractive'\n",
        "\n",
        "print(\"üì¶ Instalando dependencias del sistema...\")\n",
        "!sudo apt-get update -qq 2>/dev/null\n",
        "!sudo apt-get install -y -qq zstd pciutils 2>/dev/null\n",
        "\n",
        "print(\"\\nüì• Instalando Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh 2>&1 | grep -v 'systemd'\n",
        "\n",
        "!ollama --version"
      ],
      "metadata": {
        "id": "80CVXI0z30OR",
        "outputId": "bafdf693-396b-4dfb-c72c-2ad43ffb904f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Instalando dependencias del sistema...\n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 121852 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Selecting previously unselected package zstd.\n",
            "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
            "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "\n",
            "üì• Instalando Ollama...\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Warning: could not connect to a running Ollama instance\n",
            "Warning: client version is 0.16.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================================\n",
        "# PASO 2: Instalar librer√≠as Python\n",
        "# ===========================================================================\n",
        "# - pyngrok: cliente Python para el t√∫nel ngrok\n",
        "# - ollama: cliente oficial para la API de Ollama\n",
        "# - langchain + langchain-ollama: framework de orquestaci√≥n de LLMs\n",
        "# - langchain-community: integraciones de terceros (vectorstores, loaders, etc.)\n",
        "# - faiss-cpu: librer√≠a de b√∫squeda vectorial eficiente (de Meta)\n",
        "#   Usamos la versi√≥n CPU porque FAISS corre en RAM, no en VRAM\n",
        "\n",
        "print(\"üì¶ Instalando librer√≠as Python...\")\n",
        "!pip install -qq pyngrok ollama\n",
        "!pip install -qq langchain langchain-ollama langchain-community\n",
        "!pip install -qq faiss-cpu"
      ],
      "metadata": {
        "id": "Y4lP8hTY4HJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================================\n",
        "# PASO 3: Configurar ngrok\n",
        "# ===========================================================================\n",
        "# ngrok crea un t√∫nel HTTPS entre internet y un puerto local de Colab.\n",
        "# Necesitamos esto porque Colab no tiene IP p√∫blica directa.\n",
        "#\n",
        "# Configura tu token en Colab Secrets (icono üîë en el panel izquierdo):\n",
        "#   Nombre: NGROK_AUTHTOKEN\n",
        "#   Valor: tu token de https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    NGROK_AUTHTOKEN = userdata.get('NGROK_AUTHTOKEN')\n",
        "    print(f\"‚úÖ Token cargado: {NGROK_AUTHTOKEN[:8]}...{NGROK_AUTHTOKEN[-4:]}\")\n",
        "except Exception:\n",
        "    raise ValueError(\n",
        "        \"‚ùå Token de ngrok no encontrado.\\n\"\n",
        "        \"A√±√°delo en Colab Secrets (üîë) con el nombre NGROK_AUTHTOKEN.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "T5GAwBf54J7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================================\n",
        "# PASO 4: Iniciar el servidor Ollama\n",
        "# ===========================================================================\n",
        "# Lanzamos Ollama como proceso en background.\n",
        "# OLLAMA_HOST=0.0.0.0 es necesario para que ngrok pueda alcanzarlo.\n",
        "\n",
        "import subprocess, time, requests\n",
        "\n",
        "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "\n",
        "print(\"üöÄ Iniciando servidor Ollama...\")\n",
        "ollama_proc = subprocess.Popen(\n",
        "    ['ollama', 'serve'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "time.sleep(5)\n",
        "\n",
        "# Verificar que el servidor responde\n",
        "for intento in range(3):\n",
        "    try:\n",
        "        r = requests.get('http://localhost:11434', timeout=10)\n",
        "        print(f\"‚úÖ Servidor activo: {r.text.strip()}\")\n",
        "        break\n",
        "    except requests.ConnectionError:\n",
        "        print(f\"   Intento {intento + 1}/3... esperando\")\n",
        "        time.sleep(5)\n",
        "else:\n",
        "    raise RuntimeError(\"‚ùå El servidor Ollama no arranc√≥ correctamente.\")\n"
      ],
      "metadata": {
        "id": "enR0M9JW4WxA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}